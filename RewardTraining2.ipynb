{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bae8ee15-2469-416b-b09b-24a18308b7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "from operator import itemgetter\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import AutoModelForSequenceClassification,AutoTokenizer,TrainingArguments\n",
    "from trl import RewardTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c751083b-a650-4fda-9ca3-3939950ae23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              prompt  \\\n",
      "0  <bos><start_of_turn>user\\nGiven the mission ba...   \n",
      "1  <bos><start_of_turn>user\\nGiven the mission ba...   \n",
      "2  <bos><start_of_turn>user\\nGiven the mission ba...   \n",
      "3  <bos><start_of_turn>user\\nGiven the mission ba...   \n",
      "4  <bos><start_of_turn>user\\nGiven the mission ba...   \n",
      "\n",
      "                                          completion  label  \n",
      "0  ## Operation Crimson Echo - Success Scenario\\n...   True  \n",
      "1  ## Operation Crimson Echo - Success Scenario\\n...   True  \n",
      "2  ## Operation Crimson Echo - Success Scenario\\n...   True  \n",
      "3  ## Operation Crimson Echo - Success Scenario\\n...   True  \n",
      "4  ## Operation Crimson Echo - Success Scenario\\n...   True  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('scenarioData3_llm_reward.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3126c027-4b7a-46e8-856e-c0831dc89bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion_lst = df['completion'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ef69d13-bcc7-4874-8831-a9769e70ada3",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(columns=['chosen_response', 'rejected_response'])\n",
    "chosen_lst = []\n",
    "rejected_lst = [] \n",
    "\n",
    "for i in range(50): \n",
    "    chosen_lst.append(completion_lst[i])\n",
    "    rejected_lst.append(completion_lst[i+50])\n",
    "\n",
    "new_df['chosen_response'] = chosen_lst\n",
    "new_df['rejected_response'] = rejected_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7670a89a-9b1e-4c6d-84c1-79ff0ab9ab17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     chosen_response  \\\n",
      "0  ## Operation Crimson Echo - Success Scenario\\n...   \n",
      "1  ## Operation Crimson Echo - Success Scenario\\n...   \n",
      "2  ## Operation Crimson Echo - Success Scenario\\n...   \n",
      "3  ## Operation Crimson Echo - Success Scenario\\n...   \n",
      "4  ## Operation Crimson Echo - Success Scenario\\n...   \n",
      "\n",
      "                                   rejected_response  \n",
      "0  ## Operation Crimson Echo - Success Scenario\\n...  \n",
      "1  ## Operation Crimson Echo - Success Scenario\\n...  \n",
      "2  ## Operation Crimson Echo - Success Scenario\\n...  \n",
      "3  ## Operation Crimson Echo - Success Scenario\\n...  \n",
      "4  ## Operation Crimson Echo - Success Scenario\\n...  \n"
     ]
    }
   ],
   "source": [
    "print(new_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "337db60d-91f1-496a-8304-3e8234fda40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#Select a base model whch we need to train for reward modeling.\n",
    "model_name = \"distilroberta-base\"\n",
    "#model_name = \"allenai/longformer-base-4096\"\n",
    "#model_name = \"sshleifer/distilbart-cnn-12-6\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9a16349-1edc-4617-a877-075c3f87270a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func(row):\n",
    "    kwargs = {\"padding\": \"max_length\", \"truncation\": True, \"max_length\": 512, \"return_tensors\": \"pt\"}\n",
    "    #kwargs = {\"padding\": \"max_length\", \"truncation\": True, \"max_length\": 2048, \"return_tensors\": \"pt\"}\n",
    "    prompt_plus_chosen_response = row[\"chosen_response\"]\n",
    "    prompt_plus_rejected_response = row[\"rejected_response\"]\n",
    "    tokens_chosen = tokenizer.encode_plus(prompt_plus_chosen_response, **kwargs)\n",
    "    tokens_rejected = tokenizer.encode_plus(prompt_plus_rejected_response, **kwargs)\n",
    "    return {\n",
    "        \"input_ids_chosen\": tokens_chosen[\"input_ids\"][0], \"attention_mask_chosen\": tokens_chosen[\"attention_mask\"][0],\n",
    "        \"input_ids_rejected\": tokens_rejected[\"input_ids\"][0], \"attention_mask_rejected\": tokens_rejected[\"attention_mask\"][0]\n",
    "    }\n",
    "    \n",
    "formatted_dataset = new_df.apply(formatting_func, axis=1)\n",
    "formatted_df= pd.DataFrame(list(formatted_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcd47901-256c-409b-86b5-9fa50639c642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids_chosen           [tensor(0), tensor(48342), tensor(13346), tens...\n",
      "attention_mask_chosen      [tensor(1), tensor(1), tensor(1), tensor(1), t...\n",
      "input_ids_rejected         [tensor(0), tensor(48342), tensor(13346), tens...\n",
      "attention_mask_rejected    [tensor(1), tensor(1), tensor(1), tensor(1), t...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(formatted_df.iloc[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c28c1ec-aad5-4f8d-a4fc-902836bdaf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "formatted_dict = {\n",
    "    'input_ids_chosen': formatted_df['input_ids_chosen'].tolist(), \n",
    "    'attention_mask_chosen': formatted_df['attention_mask_chosen'].tolist(), \n",
    "    'input_ids_rejected': formatted_df['input_ids_rejected'].tolist(), \n",
    "    'attention_mask_rejected': formatted_df['attention_mask_rejected'].tolist()\n",
    "}\n",
    "formatted_dataset = Dataset.from_dict(formatted_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b685aa2-d83f-4a39-baf7-45829284c0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected', 'attention_mask_rejected'],\n",
      "    num_rows: 50\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(formatted_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3eb3152a-556c-4cfd-a7e6-62e1946154ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e869a689-37cb-4326-ad2c-a2c1c0956e30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "wandb: Currently logged in as: pkailin2002 (nus-pkailin). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Development (UC)\\Desktop\\kailin\\Review3\\wandb\\run-20240719_161728-5scm0pkf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nus-pkailin/huggingface/runs/5scm0pkf' target=\"_blank\">./reward_model2_4</a></strong> to <a href='https://wandb.ai/nus-pkailin/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nus-pkailin/huggingface' target=\"_blank\">https://wandb.ai/nus-pkailin/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nus-pkailin/huggingface/runs/5scm0pkf' target=\"_blank\">https://wandb.ai/nus-pkailin/huggingface/runs/5scm0pkf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 55:48, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.683700</td>\n",
       "      <td>0.693043</td>\n",
       "      <td>0.538462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.713800</td>\n",
       "      <td>0.692930</td>\n",
       "      <td>0.538462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.704100</td>\n",
       "      <td>0.692681</td>\n",
       "      <td>0.692308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.687300</td>\n",
       "      <td>0.692344</td>\n",
       "      <td>0.769231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.702000</td>\n",
       "      <td>0.691867</td>\n",
       "      <td>0.846154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.678600</td>\n",
       "      <td>0.691323</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.684600</td>\n",
       "      <td>0.690630</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.677200</td>\n",
       "      <td>0.689844</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.682400</td>\n",
       "      <td>0.688861</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.678700</td>\n",
       "      <td>0.687809</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.693200</td>\n",
       "      <td>0.686511</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.699800</td>\n",
       "      <td>0.685167</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.686400</td>\n",
       "      <td>0.683581</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.673700</td>\n",
       "      <td>0.681819</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.684400</td>\n",
       "      <td>0.679707</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.668200</td>\n",
       "      <td>0.677266</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.663900</td>\n",
       "      <td>0.674189</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.665200</td>\n",
       "      <td>0.670837</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.655600</td>\n",
       "      <td>0.666523</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.662400</td>\n",
       "      <td>0.661610</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.654300</td>\n",
       "      <td>0.655378</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.638600</td>\n",
       "      <td>0.648009</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.631300</td>\n",
       "      <td>0.638872</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.593000</td>\n",
       "      <td>0.628003</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.612300</td>\n",
       "      <td>0.613934</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.612500</td>\n",
       "      <td>0.597669</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.566700</td>\n",
       "      <td>0.577379</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.588000</td>\n",
       "      <td>0.553876</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.503700</td>\n",
       "      <td>0.525435</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.495500</td>\n",
       "      <td>0.493654</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.460500</td>\n",
       "      <td>0.459273</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.455500</td>\n",
       "      <td>0.427380</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.402500</td>\n",
       "      <td>0.400740</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.333200</td>\n",
       "      <td>0.375906</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.320700</td>\n",
       "      <td>0.348806</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.252000</td>\n",
       "      <td>0.317375</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.271200</td>\n",
       "      <td>0.283666</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.200200</td>\n",
       "      <td>0.249913</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.200700</td>\n",
       "      <td>0.218121</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.212200</td>\n",
       "      <td>0.192061</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.164000</td>\n",
       "      <td>0.172858</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.098600</td>\n",
       "      <td>0.160657</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.119600</td>\n",
       "      <td>0.154952</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.081700</td>\n",
       "      <td>0.144188</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.082800</td>\n",
       "      <td>0.126964</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.045900</td>\n",
       "      <td>0.105608</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.058400</td>\n",
       "      <td>0.092536</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.036500</td>\n",
       "      <td>0.084849</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.036400</td>\n",
       "      <td>0.082309</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.084780</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.094017</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.016500</td>\n",
       "      <td>0.096631</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.025500</td>\n",
       "      <td>0.091493</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.013200</td>\n",
       "      <td>0.081482</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.015700</td>\n",
       "      <td>0.075826</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.008700</td>\n",
       "      <td>0.069405</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.012100</td>\n",
       "      <td>0.066450</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.009200</td>\n",
       "      <td>0.063347</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.012100</td>\n",
       "      <td>0.062852</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.011600</td>\n",
       "      <td>0.062628</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>0.062136</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.074700</td>\n",
       "      <td>0.063616</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.007800</td>\n",
       "      <td>0.065505</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.067247</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>0.069111</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.005700</td>\n",
       "      <td>0.070508</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.006900</td>\n",
       "      <td>0.072232</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>0.071707</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>0.069948</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.010300</td>\n",
       "      <td>0.070978</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>0.071908</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>0.071319</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>0.070794</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>0.069488</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>0.067958</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>0.065219</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.063106</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>0.061552</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.060472</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>0.059586</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>0.059135</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.058783</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>0.058744</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.058702</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>0.058763</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.058806</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>0.058960</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>0.059051</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.059153</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.059227</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>0.059301</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.059353</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.059406</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.059439</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>0.059464</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.059476</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>0.059479</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>0.059492</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.059498</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.059498</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=0.2382880789099727, metrics={'train_runtime': 3396.9533, 'train_samples_per_second': 0.545, 'train_steps_per_second': 0.029, 'total_flos': 0.0, 'train_loss': 0.2382880789099727, 'epoch': 50.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_dataset = formatted_dataset.train_test_split()\n",
    "# Configuring the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./reward_model2_4\",\n",
    "    per_device_train_batch_size=32,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_steps=1,\n",
    "    num_train_epochs=50,\n",
    "    report_to=None,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\", \n",
    "    learning_rate=1e-5,\n",
    "    no_cuda=True\n",
    ")\n",
    "\n",
    "# Loading the RewardTrainer from TRL\n",
    "trainer = RewardTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=formatted_dataset[\"train\"],\n",
    "    eval_dataset=formatted_dataset[\"test\"],\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc864fe1-36cd-469f-8871-75898666abab",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./rewardmodel5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d7e7a4-638f-4e6f-a8ae-ac9cdffde77b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
